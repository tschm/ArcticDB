{
    "basic_functions.BasicFunctions.peakmem_read": {
        "code": "class BasicFunctions:\n    def peakmem_read(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        [lib.read(f\"{sym}_sym\").data for sym in range(num_symbols)]\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n    \n        lib = get_prewritten_lib_name(5_000)\n        self.ac.delete_library(lib)\n        self.ac.create_library(lib)\n        lib = self.ac[lib]\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))\n    \n        del self.ac",
        "name": "basic_functions.BasicFunctions.peakmem_read",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "296f09c98267201c1783cbc9586a946b71a60558ff3e8b8bac9c300595649dc3"
    },
    "basic_functions.BasicFunctions.peakmem_read_batch": {
        "code": "class BasicFunctions:\n    def peakmem_read_batch(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n        lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n    \n        lib = get_prewritten_lib_name(5_000)\n        self.ac.delete_library(lib)\n        self.ac.create_library(lib)\n        lib = self.ac[lib]\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))\n    \n        del self.ac",
        "name": "basic_functions.BasicFunctions.peakmem_read_batch",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "2aa798adcef8847939425589397eaea2d5bd2b4c174924471ca4c148c6c94cc1"
    },
    "basic_functions.BasicFunctions.peakmem_read_batch_with_columns": {
        "code": "class BasicFunctions:\n    def peakmem_read_batch_with_columns(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        COLS = [\"value\"]\n        read_reqs = [\n            ReadRequest(f\"{sym}_sym\", columns=COLS) for sym in range(num_symbols)\n        ]\n        lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n    \n        lib = get_prewritten_lib_name(5_000)\n        self.ac.delete_library(lib)\n        self.ac.create_library(lib)\n        lib = self.ac[lib]\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))\n    \n        del self.ac",
        "name": "basic_functions.BasicFunctions.peakmem_read_batch_with_columns",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "a1f0e2b63d02471d41cb9baaf7a5cbc7df80fa382f98605a222475c1e055ac89"
    },
    "basic_functions.BasicFunctions.peakmem_read_batch_with_date_ranges": {
        "code": "class BasicFunctions:\n    def peakmem_read_batch_with_date_ranges(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        dr = pd.date_range(\"2023-01-01\", \"2023-01-01\")\n        read_reqs = [\n            ReadRequest(f\"{sym}_sym\", date_range=dr) for sym in range(num_symbols)\n        ]\n        lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n    \n        lib = get_prewritten_lib_name(5_000)\n        self.ac.delete_library(lib)\n        self.ac.create_library(lib)\n        lib = self.ac[lib]\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))\n    \n        del self.ac",
        "name": "basic_functions.BasicFunctions.peakmem_read_batch_with_date_ranges",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "74a7dfa421cd220a393c906d8fdecb7f6b6962ee41b8c10266ba839d3fca7a19"
    },
    "basic_functions.BasicFunctions.peakmem_read_short_wide": {
        "code": "class BasicFunctions:\n    def peakmem_read_short_wide(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(5000)]\n        lib.read(\"short_wide_sym\").data\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n    \n        lib = get_prewritten_lib_name(5_000)\n        self.ac.delete_library(lib)\n        self.ac.create_library(lib)\n        lib = self.ac[lib]\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))\n    \n        del self.ac",
        "name": "basic_functions.BasicFunctions.peakmem_read_short_wide",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "2c1d80e9fbadc31cab7d9a09e429fbd85ac11d72b2b4b0c7aa5a4abd39158b6d"
    },
    "basic_functions.BasicFunctions.peakmem_read_with_columns": {
        "code": "class BasicFunctions:\n    def peakmem_read_with_columns(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        COLS = [\"value\"]\n        [lib.read(f\"{sym}_sym\", columns=COLS).data for sym in range(num_symbols)]\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n    \n        lib = get_prewritten_lib_name(5_000)\n        self.ac.delete_library(lib)\n        self.ac.create_library(lib)\n        lib = self.ac[lib]\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))\n    \n        del self.ac",
        "name": "basic_functions.BasicFunctions.peakmem_read_with_columns",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "8ec03c2dea81194337db6fa034376731e07e290b0ce9243ddbe9f299a5f87c8a"
    },
    "basic_functions.BasicFunctions.peakmem_read_with_date_ranges": {
        "code": "class BasicFunctions:\n    def peakmem_read_with_date_ranges(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        dr = pd.date_range(\"2023-01-01\", \"2023-01-01\")\n        [lib.read(f\"{sym}_sym\", date_range=dr).data for sym in range(num_symbols)]\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n    \n        lib = get_prewritten_lib_name(5_000)\n        self.ac.delete_library(lib)\n        self.ac.create_library(lib)\n        lib = self.ac[lib]\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))\n    \n        del self.ac",
        "name": "basic_functions.BasicFunctions.peakmem_read_with_date_ranges",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "bccbe8af0b5295638890c6b54dd2cf65cff75e2815e5a37fcd78ee772c57d0b2"
    },
    "basic_functions.BasicFunctions.peakmem_write": {
        "code": "class BasicFunctions:\n    def peakmem_write(self, rows, num_symbols):\n        lib = self.get_fresh_lib()\n        for sym in range(num_symbols):\n            lib.write(f\"{sym}_sym\", self.df)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n    \n        lib = get_prewritten_lib_name(5_000)\n        self.ac.delete_library(lib)\n        self.ac.create_library(lib)\n        lib = self.ac[lib]\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))\n    \n        del self.ac",
        "name": "basic_functions.BasicFunctions.peakmem_write",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "e9fac0d0bf16de588db927fd9437f43cd2772a4e95a531374f40a3c7970eacc8"
    },
    "basic_functions.BasicFunctions.peakmem_write_batch": {
        "code": "class BasicFunctions:\n    def peakmem_write_batch(self, rows, num_symbols):\n        lib = self.get_fresh_lib()\n        df = self.df\n        payloads = [WritePayload(f\"{sym}_sym\", df) for sym in range(num_symbols)]\n        lib.write_batch(payloads)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n    \n        lib = get_prewritten_lib_name(5_000)\n        self.ac.delete_library(lib)\n        self.ac.create_library(lib)\n        lib = self.ac[lib]\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))\n    \n        del self.ac",
        "name": "basic_functions.BasicFunctions.peakmem_write_batch",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "860b82de9e01a110f7c16b4df2e0ce779e4c52390272db8693ee0fffd49ce9a2"
    },
    "basic_functions.BasicFunctions.peakmem_write_short_wide": {
        "code": "class BasicFunctions:\n    def peakmem_write_short_wide(self, rows, num_symbols):\n        lib = self.get_fresh_lib()\n        lib.write(\"short_wide_sym\", self.df_short_wide)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n    \n        lib = get_prewritten_lib_name(5_000)\n        self.ac.delete_library(lib)\n        self.ac.create_library(lib)\n        lib = self.ac[lib]\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))\n    \n        del self.ac",
        "name": "basic_functions.BasicFunctions.peakmem_write_short_wide",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "945c5feb1846a05b4c8cb7c7ab59672b6e6f37a6c71fa41df77333a46dea1a52"
    },
    "basic_functions.BasicFunctions.peakmem_write_staged": {
        "code": "class BasicFunctions:\n    def peakmem_write_staged(self, rows, num_symbols):\n        lib = self.get_fresh_lib()\n        for sym in range(num_symbols):\n            lib.write(f\"{sym}_sym\", self.df, staged=True)\n    \n        for sym in range(num_symbols):\n            lib._nvs.compact_incomplete(f\"{sym}_sym\", False, False)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n    \n        lib = get_prewritten_lib_name(5_000)\n        self.ac.delete_library(lib)\n        self.ac.create_library(lib)\n        lib = self.ac[lib]\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))\n    \n        del self.ac",
        "name": "basic_functions.BasicFunctions.peakmem_write_staged",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "9d6ec5033da44ae4bc65e1ff969d83521bd6669119677badf1147689d6f0d52a"
    },
    "basic_functions.BasicFunctions.time_read": {
        "code": "class BasicFunctions:\n    def time_read(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        [lib.read(f\"{sym}_sym\").data for sym in range(num_symbols)]\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n    \n        lib = get_prewritten_lib_name(5_000)\n        self.ac.delete_library(lib)\n        self.ac.create_library(lib)\n        lib = self.ac[lib]\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))\n    \n        del self.ac",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "e23afb962f382d31a376d24887967b28bb5ae38c39b9b62636d43a8ffbb09acd",
        "warmup_time": -1
    },
    "basic_functions.BasicFunctions.time_read_batch": {
        "code": "class BasicFunctions:\n    def time_read_batch(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n        lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n    \n        lib = get_prewritten_lib_name(5_000)\n        self.ac.delete_library(lib)\n        self.ac.create_library(lib)\n        lib = self.ac[lib]\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))\n    \n        del self.ac",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_batch",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "509e193687f21aa3751e31c91b670e96959a368f05a49207f92b5f43c3959aac",
        "warmup_time": -1
    },
    "basic_functions.BasicFunctions.time_read_batch_pure": {
        "code": "class BasicFunctions:\n    def time_read_batch_pure(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        lib.read_batch(self.read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n    \n        lib = get_prewritten_lib_name(5_000)\n        self.ac.delete_library(lib)\n        self.ac.create_library(lib)\n        lib = self.ac[lib]\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))\n    \n        del self.ac",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_batch_pure",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "4841b8375c415f347c92eb0b588cd984f8fa760867718bf2107ce4382b9468e9",
        "warmup_time": -1
    },
    "basic_functions.BasicFunctions.time_read_batch_with_columns": {
        "code": "class BasicFunctions:\n    def time_read_batch_with_columns(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        COLS = [\"value\"]\n        read_reqs = [\n            ReadRequest(f\"{sym}_sym\", columns=COLS) for sym in range(num_symbols)\n        ]\n        lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n    \n        lib = get_prewritten_lib_name(5_000)\n        self.ac.delete_library(lib)\n        self.ac.create_library(lib)\n        lib = self.ac[lib]\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))\n    \n        del self.ac",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_batch_with_columns",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "93eab7b36aa9a0b4eb723a196d6b1b215d3e03b03d795035e5d5b5eef9ee25ac",
        "warmup_time": -1
    },
    "basic_functions.BasicFunctions.time_read_batch_with_date_ranges": {
        "code": "class BasicFunctions:\n    def time_read_batch_with_date_ranges(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        dr = pd.date_range(\"2023-01-01\", \"2023-01-01\")\n        read_reqs = [\n            ReadRequest(f\"{sym}_sym\", date_range=dr) for sym in range(num_symbols)\n        ]\n        lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n    \n        lib = get_prewritten_lib_name(5_000)\n        self.ac.delete_library(lib)\n        self.ac.create_library(lib)\n        lib = self.ac[lib]\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))\n    \n        del self.ac",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_batch_with_date_ranges",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "2e3b02b0b94b492b2fe24a3d9ff65e71994032b227fea5810196f179264e156f",
        "warmup_time": -1
    },
    "basic_functions.BasicFunctions.time_read_short_wide": {
        "code": "class BasicFunctions:\n    def time_read_short_wide(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(5000)]\n        lib.read(\"short_wide_sym\").data\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n    \n        lib = get_prewritten_lib_name(5_000)\n        self.ac.delete_library(lib)\n        self.ac.create_library(lib)\n        lib = self.ac[lib]\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))\n    \n        del self.ac",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_short_wide",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "4789b111c8962a3b76ed75bc9f1197b14524eb96b5ab264216a91fdf5523d5a1",
        "warmup_time": -1
    },
    "basic_functions.BasicFunctions.time_read_with_columns": {
        "code": "class BasicFunctions:\n    def time_read_with_columns(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        COLS = [\"value\"]\n        [lib.read(f\"{sym}_sym\", columns=COLS).data for sym in range(num_symbols)]\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n    \n        lib = get_prewritten_lib_name(5_000)\n        self.ac.delete_library(lib)\n        self.ac.create_library(lib)\n        lib = self.ac[lib]\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))\n    \n        del self.ac",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_with_columns",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "a1a79214106f1290813f2b533aa7b976a1f8d28b6a2ba1b553498e75b39a9e4e",
        "warmup_time": -1
    },
    "basic_functions.BasicFunctions.time_read_with_date_ranges": {
        "code": "class BasicFunctions:\n    def time_read_with_date_ranges(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        dr = pd.date_range(\"2023-01-01\", \"2023-01-01\")\n        [lib.read(f\"{sym}_sym\", date_range=dr).data for sym in range(num_symbols)]\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n    \n        lib = get_prewritten_lib_name(5_000)\n        self.ac.delete_library(lib)\n        self.ac.create_library(lib)\n        lib = self.ac[lib]\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))\n    \n        del self.ac",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_with_date_ranges",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "fb7eb7cfa6f193eca19d8f580605c87ee4d416e6575d8cc44c693c91771640a9",
        "warmup_time": -1
    },
    "basic_functions.BasicFunctions.time_write": {
        "code": "class BasicFunctions:\n    def time_write(self, rows, num_symbols):\n        lib = self.get_fresh_lib()\n        for sym in range(num_symbols):\n            lib.write(f\"{sym}_sym\", self.df)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n    \n        lib = get_prewritten_lib_name(5_000)\n        self.ac.delete_library(lib)\n        self.ac.create_library(lib)\n        lib = self.ac[lib]\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))\n    \n        del self.ac",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_write",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "e1835ceba2055c122d77f88c03873240154086b26a3f726ecaeee1e1a86965b2",
        "warmup_time": -1
    },
    "basic_functions.BasicFunctions.time_write_batch": {
        "code": "class BasicFunctions:\n    def time_write_batch(self, rows, num_symbols):\n        lib = self.get_fresh_lib()\n        df = self.df\n        payloads = [WritePayload(f\"{sym}_sym\", df) for sym in range(num_symbols)]\n        lib.write_batch(payloads)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n    \n        lib = get_prewritten_lib_name(5_000)\n        self.ac.delete_library(lib)\n        self.ac.create_library(lib)\n        lib = self.ac[lib]\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))\n    \n        del self.ac",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_write_batch",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "32a4579d9ca286f65352715a81fa9c3e1b1e41c4bbc5fff0aefeb6a14ca17fe4",
        "warmup_time": -1
    },
    "basic_functions.BasicFunctions.time_write_short_wide": {
        "code": "class BasicFunctions:\n    def time_write_short_wide(self, rows, num_symbols):\n        lib = self.get_fresh_lib()\n        lib.write(\"short_wide_sym\", self.df_short_wide)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n    \n        lib = get_prewritten_lib_name(5_000)\n        self.ac.delete_library(lib)\n        self.ac.create_library(lib)\n        lib = self.ac[lib]\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))\n    \n        del self.ac",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_write_short_wide",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "eb63c9988cee3cde582ab70100236ab0d1f92641d18db383d2f3fe4ba0189032",
        "warmup_time": -1
    },
    "basic_functions.BasicFunctions.time_write_staged": {
        "code": "class BasicFunctions:\n    def time_write_staged(self, rows, num_symbols):\n        lib = self.get_fresh_lib()\n        for sym in range(num_symbols):\n            lib.write(f\"{sym}_sym\", self.df, staged=True)\n    \n        for sym in range(num_symbols):\n            lib._nvs.compact_incomplete(f\"{sym}_sym\", False, False)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=20GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n    \n        lib = get_prewritten_lib_name(5_000)\n        self.ac.delete_library(lib)\n        self.ac.create_library(lib)\n        lib = self.ac[lib]\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))\n    \n        del self.ac",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_write_staged",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "4993e5e90468dc91cbdf9a161325d3563e72fd3894fe2eb70dfd2d07f4b77b68",
        "warmup_time": -1
    },
    "list_functions.ListFunctions.peakmem_list_symbols": {
        "code": "class ListFunctions:\n    def peakmem_list_symbols(self, num_symbols):\n        lib = self.ac[f\"{num_symbols}_num_symbols\"]\n        lib.list_symbols()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            self.ac.create_library(lib_name)\n            lib = self.ac[lib_name]\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))",
        "name": "list_functions.ListFunctions.peakmem_list_symbols",
        "param_names": [
            "num_symbols"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "list_functions:22",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "0d50729844beae58e9cba083ee7214a55e5c1639b8439842ed1cdf080b7b2a4f"
    },
    "list_functions.ListFunctions.peakmem_list_versions": {
        "code": "class ListFunctions:\n    def peakmem_list_versions(self, num_symbols):\n        lib = self.ac[f\"{num_symbols}_num_symbols\"]\n        lib.list_versions()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            self.ac.create_library(lib_name)\n            lib = self.ac[lib_name]\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))",
        "name": "list_functions.ListFunctions.peakmem_list_versions",
        "param_names": [
            "num_symbols"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "list_functions:22",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "c317f2aa935e2b739a1a232439412ad1da88ee9eb332daf0f560e00ec27f1143"
    },
    "list_functions.ListFunctions.time_has_symbol": {
        "code": "class ListFunctions:\n    def time_has_symbol(self, num_symbols):\n        lib = self.ac[f\"{num_symbols}_num_symbols\"]\n        lib.has_symbol(\"250_sym\")\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            self.ac.create_library(lib_name)\n            lib = self.ac[lib_name]\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))",
        "min_run_count": 2,
        "name": "list_functions.ListFunctions.time_has_symbol",
        "number": 5,
        "param_names": [
            "num_symbols"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "list_functions:22",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "8a8e9e8927b3b8698d1ef4bab4119455588fc62a3bbfa62321e6ce83f5f428ad",
        "warmup_time": -1
    },
    "list_functions.ListFunctions.time_list_symbols": {
        "code": "class ListFunctions:\n    def time_list_symbols(self, num_symbols):\n        lib = self.ac[f\"{num_symbols}_num_symbols\"]\n        lib.list_symbols()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            self.ac.create_library(lib_name)\n            lib = self.ac[lib_name]\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))",
        "min_run_count": 2,
        "name": "list_functions.ListFunctions.time_list_symbols",
        "number": 5,
        "param_names": [
            "num_symbols"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "list_functions:22",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "c91930aa368c8d3ad7bd56d7f827edcca3f16645c205bf6e042c57a5792a7113",
        "warmup_time": -1
    },
    "list_functions.ListFunctions.time_list_versions": {
        "code": "class ListFunctions:\n    def time_list_versions(self, num_symbols):\n        lib = self.ac[f\"{num_symbols}_num_symbols\"]\n        lib.list_versions()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            self.ac.create_library(lib_name)\n            lib = self.ac[lib_name]\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))",
        "min_run_count": 2,
        "name": "list_functions.ListFunctions.time_list_versions",
        "number": 5,
        "param_names": [
            "num_symbols"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "list_functions:22",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "ed4587ca351d24477560baa8e184e265b612140183e88c412c36bf8a9352191c",
        "warmup_time": -1
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_numeric": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_filtering_numeric(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 10.0]\n        lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_numeric",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:21",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "119932c4738eb092fbbd9da22b3ff1e16bd4d6052dace2da6748fff5b416462d"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_string_isin": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_filtering_string_isin(self, num_rows):\n        lib = self.ac[self.lib_name]\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_string_isin",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:21",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "e77e28f6a2cb96e5b549ca58b551462e0ff90d7704ad3cf12f3e2ad71b1ccd73"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_projection": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_projection(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        lib.read(f\"{num_rows}_rows\", columns=[\"new_col\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_projection",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:21",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "928c58e85b09b5969e7b2302b3a3a3c5af5d47ce5bd1cd5245387bf34fcb266b"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_1": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_1(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_1",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:21",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "a1d501b169474fd16fdb1e90760157944483bba8d81eb3cb5791bbd487706ef6"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_3": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_3(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_3",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:21",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "0b19c5c0e4be8742e99e32f13332544c343ac260474ee51bfb8911225c90b87b"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_4": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_4(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_4",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:21",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "76d23e33cb5a319de6d0e007054bc0c3588cde135a08797e8f0c84493ed30189"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_adv_query_2": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_adv_query_2(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_adv_query_2",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:21",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "36489c1cb0a3965ff6003b10a5f5cf2f2354a619346e53c488b2a3d1a07fd4c2"
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_filtering_numeric": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_filtering_numeric(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 1.0]\n        lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_filtering_numeric",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:21",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "441c9db2c41da7c09c3f758c1cd0993dc5ea674f05d708e8997567826a961251",
        "warmup_time": -1
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_isin": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_filtering_string_isin(self, num_rows):\n        lib = self.ac[self.lib_name]\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_isin",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:21",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "f9d819943b527f80d002995694ba8cfc60f5ed75586144f6c4d1ba612a761294",
        "warmup_time": -1
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_projection": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_projection(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        lib.read(f\"{num_rows}_rows\", columns=[\"new_col\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_projection",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:21",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "911828bde306e54bd165122188fb8acc406697e8cc94daa3e02596c1efc4530a",
        "warmup_time": -1
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_query_1": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_query_1(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_1",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:21",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "e0d8b8b13b6012a8fe388a57e1d816fc387312c6595ff5cbf0490ae32a11642e",
        "warmup_time": -1
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_query_3": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_query_3(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_3",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:21",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "22c320dbb7f3915712bffffef0c86ec9a51a167a8aee1af369ae9afceb47ad0b",
        "warmup_time": -1
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_query_4": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_query_4(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_4",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:21",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "f6c81bfbe6bd6a3ed5de7ede40c048ee2e7f0f2f844608169ddf1768a18aa900",
        "warmup_time": -1
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_query_adv_query_2": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_query_adv_query_2(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_adv_query_2",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:21",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "8167772e10cd53cd18fa670b57287145bdee6a010f1f8e07c9beec045706692e",
        "warmup_time": -1
    },
    "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_1": {
        "code": "class PersistentQueryBuilderFunctions:\n    def time_query_1(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        pass\n\n    def setup_cache(self):\n        self.ac = Arctic(get_real_s3_uri())\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = \"query_builder_benchmark_lib\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_1",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "persistent_query_builder:61",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "ba7a44067694a7893735473723fe49cef5bf05243910f164067ecf4cc6a64900",
        "warmup_time": -1
    },
    "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_3": {
        "code": "class PersistentQueryBuilderFunctions:\n    def time_query_3(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        pass\n\n    def setup_cache(self):\n        self.ac = Arctic(get_real_s3_uri())\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = \"query_builder_benchmark_lib\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_3",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "persistent_query_builder:61",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "dc9b55830328255d89a5e2ad08c591e82a6c83a82842f145c5fcea00a4431592",
        "warmup_time": -1
    },
    "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_4": {
        "code": "class PersistentQueryBuilderFunctions:\n    def time_query_4(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        pass\n\n    def setup_cache(self):\n        self.ac = Arctic(get_real_s3_uri())\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = \"query_builder_benchmark_lib\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_4",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "persistent_query_builder:61",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "9f659b0cc27082fe5e3083fbfcd7d0011709c7614fd59aca394fad7c0fe62bce",
        "warmup_time": -1
    },
    "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_adv_query_2": {
        "code": "class PersistentQueryBuilderFunctions:\n    def time_query_adv_query_2(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        pass\n\n    def setup_cache(self):\n        self.ac = Arctic(get_real_s3_uri())\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = \"query_builder_benchmark_lib\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_adv_query_2",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "persistent_query_builder:61",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "00d25064569b0d4da1ba149a5cabb7c0244f2fd107ff5c22ea1bbceef4a0f949",
        "warmup_time": -1
    },
    "version": 2
}