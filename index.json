{"project": "ArcticDB", "project_url": "http://arcticdb.io/", "show_commit_url": "https://github.com/man-group/ArcticDB/commit/", "hash_length": 8, "revision_to_hash": {"42": "8aca4e9cc678650de36a12d31d2e7f3c4cc9a03b", "214": "b70029d5724883dc903fc6a9f95b17de33dde93d", "292": "03b8e00539fc031dcabd3b2ec3accfbc9e8e3587", "308": "bf01831fd860837eec04f0437ee7ee54aca60d00", "436": "c90a21a611b5c6ec2ef4b049981ac5c2ccb8ad08", "499": "4daa3de40dd67ebde39d48797ce05240dba1e05a", "504": "14209d33c87c3325c5a77879a3cfa036894fea52", "506": "95806216ea1bdfbbc0930453adc1db42d3a4e624", "508": "525e6c28ae67faa1fe4ae2bd39d287a75fd7b083", "555": "a1b39e939f509b90a6b4f58c0c846cdfd22ae8fc", "592": "7f92cbca1bab219847ea7c9b6ccfced6b3de3565", "611": "357ead11b55196b3524196164733d2b072e09449", "620": "cf1a7326d8d648f75ec0daa3044fef4dfa71c7c5", "630": "bdf81500fef77a27f59d0541a8405ff3dfe44029", "650": "d7b6e1b40613f6b4b559a6777d1ac13e63e280bc", "699": "0a87f5c08de754cfcecf8261ad2c74119069ae65", "745": "884eedc27666ab6b208a5bb7b4cc9bdb492ff58e", "808": "3c98e789888d71133a5cccb1ad6d27a0d766f316", "815": "2f25723f0570eacc63e03fc09de6e9106b61dc4a", "828": "0f3dd263209c35022a0eac43583e11c7c38742fc", "829": "1bb8fa063f725ad00282ff18eaea3754c8b085da", "840": "09aaf7b32b5f5750e1515e7ffaea3779d5e051dc", "861": "7a449cddb6e7a2164ea03912a934d423a3925455", "891": "beab8372e6b276a57f4f6d6384b1e7bd8dc59704", "900": "7baa141aa4a73dea538d6b257dda2f334c85bb46", "916": "a54daaa294d571a2adbca6acf96edd3c3b550d96", "928": "1e1d0d70542c2df9f5dc844c037b5136f77771dd", "936": "0df1889b2e3cef53c00ccc10155186a31341e0ce", "985": "5f61b27cd7898f376418e2ff7b4e0fcc61e712fc", "996": "a7958c9c01a388a06598100d173702365cdf67cb", "1001": "74fe196747eab28dd8a78cb76cf8c6189024490b", "1002": "bc4d736c939d13cdf989ee09921877f486cd4c53", "1004": "60f55daac5e3efe6f951b718e3e9b706f9a47d58", "1020": "b6911aa5c0be36d54b878cdacf5f2df9dff1dcd9", "1023": "caaf521a4a3e543129ee99ea56de8273d31a3066", "1024": "8a9a5a41955bfdae667e8ffc7c40fde973b6fc37", "1029": "68e8f4409a4a006c77b6e255ad7ae20fa16b8d1a", "1030": "d4fb4834d5645785a2999381572456e2e1a0e2bc", "1036": "bccff006ed94d8a7530367daf67487a2ae2d471a", "1037": "c9a0b55d4dbed10135798fb20537924a2cfffa76", "1039": "7203c11b2dc111c6f5bc67d42ce66d48fdbafa05", "1047": "1e5b739d56edf61af3bb4d572706262432f72633", "1048": "4f5a6db83710619cb98645cca8b375778c3b7872", "1097": "1b925357c1cabd734d1654f838afc3e8111b1275", "1099": "044fc021b00c27a6477800666a8dd23fc679ed95", "1114": "a342cb4cbb1ad5d8d0857a1241f763c1ece8160a", "1117": "0fddd0487277e1db819cc0401a54ae44b5b03576", "1124": "5e72fd4dcd42fb97ca51b25b7ebef9a1aeb97eba", "1125": "b512348407da71fa85b9885fc82b62f0999289a1", "1130": "ed7546592a1d5da532bf805d3b9338c6d9cff2d2", "1132": "91994f71840b1b2637d5dd0a3ea16d13d3640dfb", "1150": "b785e49c97e12819d0be97a0ec2361dea1be2bd8", "1157": "5505fa4f79444037c5194f7746055b61984b1075", "1163": "8524cbdd76c99d5d4c580f0abc31fa52c9bcc922", "1164": "9989c2f0771867ea191bb0f35a28ae4183e09763", "1166": "fea52fceaf26704b003e41dede349b69057798be", "1169": "7b5853c9f38428ae84ce65c6703fd9eb03d7979d", "1170": "8a3ad1f5c842935ced5a9c99a28a93cab2042937", "1171": "a0f149818362342ce89383c7ea4458beb166d182", "1172": "56f61aa1fb4967d1608812a85cf6673ffef329df", "1187": "e283662e83e1a6373e0af19d55c29e771387719e", "1195": "8fc581f541f380293475d65a315321c8f8f38655", "1200": "8cf2a80d7c2f8539718cad038128f663a8e96810", "1208": "afd33a297b6d640e40f015506e94e64472d3334d", "1218": "1968513cdcc63e4896ccef2bc487a7cbee3d4c26", "1219": "1a19edef5ab61b3d87287f00d675f5a04de5d27b", "1225": "44ef605cb40aaa3112149af4db0106f24762053c", "1248": "f02f606e6605339756b553ed3df7118210a30a6e", "1255": "2ff4b51782e9ee4cbe50e917d2c18b1f5fbff4dc", "1257": "1f1bf0219fb0d45c1b63d67d31f98329bbf3dd30", "1277": "cd6c65446c789cd6b2c5959bf0420366024de8fa", "1278": "d76439f20ee80fafdeede5070a6a424919dfd4d1", "1279": "55b8f4a92ca1477fd6f12c59f619b7955afe7c05", "1286": "0e3c749cbcbf4de3999bba1faf9735a17d25805e", "1288": "9b81f233f2eea6cbf942909f9600e74d212b5840", "1289": "8c50cab75bb0a2360dfdbfe454ecda195a3464b3", "1299": "b935ec6e6743c944315fd671a4ab4bd9ecc4d37c", "1307": "5a38563045eb98941710984f3a8a39583f23cf2f", "1310": "982f6e7c676720679437cd9b686b748b57363326", "1315": "1dd6d95955c58000cc86cb9a38df652325c3014c", "1317": "cb85e15e7505fca49c135778429cabdfe44aba63", "1320": "8c1f9d1cd4afe70d58016678f685c23e99efef27", "1326": "aff77abb6d89effeb1e8b4c35e846562d37da10b", "1332": "27d3b7d242a5279882d3e40ec57e678812402ca5", "1333": "a1742b6e0f9feea0eb9e4ac3bc4a932cdee89d65", "1334": "6019d63ecbe9f258aa3ae0fd17ed021094b205e2", "1335": "e6d6dd89148bb8ccce5916629bbb8c89e62b89fa", "1336": "c32186fcce4542f2cfadfa0b767ccf7b17c43043", "1337": "81bd9148003f4b935079a0151370a698a0aea9f1", "1339": "fab61154299f19ad5767270649d489bfd23b165b", "1340": "580c83af9e640afd2da6129609286e16013c4112", "1341": "5ac57b50ee76147e680b289cf4101bfdd4956519", "1342": "3752ca6a353b8f44597e686c696e617de0c39a77", "1343": "69983bfea8b1c03366c5bd526c3f56b8b10efb38", "1364": "d2b57d8cfeed32808883ae0f6aedc57bc9acedd2", "1381": "ca61ee9f3e6cb95baddad84ef6565b21ab6228f7", "1383": "97eeb415a56ae5e02bcf28050a686e4e8ece3b27", "1394": "ee1a80e16e6f88fc2833f2e5fe78b4fe71bb74d6", "1395": "c4711dc40aacc050118319386125f57a8ded92c8", "1396": "45827b8ea58635dd21eb150f3b7270e999626f54", "1403": "da16be998c422dd42413e2d4262ba2b4141a420e", "1410": "a455737333a66a18383d9b9aa4bf01a21bce3846", "1415": "ce1ef0c62022ed2dd60a2e7dc4b41da63fa3a7d9", "1419": "38104424a67c61492083e103a4599c7ebb3b0b9b", "1430": "e5c626b209d61639d21a158b0d5cde1ae9b9a31c", "1434": "4867b9840013943e810b91c954eec3500606f7ce", "1436": "770faa382c1bc9fe1efaeb4dcf62324c4606deb8", "1437": "48993c110cacd0cff5a72f6190c122071748ad65", "1444": "ada51233bccdab968692e952d40f1c62323cd713", "1454": "a48cff3df9a68eb945c9cd65332ae43c1ef16d9b", "1465": "19ef5917d17d0764cf939fec510e6832fc8f1d99", "1488": "5871a9e91237eb8660ef706ca16dd43ac780667b", "1516": "218b3d9696c6a51783e508e34319e83aba326938", "1527": "262865ceafd3295d222f8cd002c4eb778cd96f80", "1529": "a40dd9c9cffe0b2dca434a15b216da0f37be4639", "1539": "456ed7eaf689564c2c184b5cf32bf79544a0192f", "1543": "837b8ffa6b232420c5868d3caded077eddc00dc4", "1545": "1a90cc53c775fe993b756fcdc8648ae643051780", "1571": "06a52de976be33b8947189a54f9bcb9763ec7c4a", "1578": "0c7373a0ceabed078ee341c906ab6cd5caeb6761", "1579": "558cc4b7d88422208f6f8a6fb811998721827a8b", "1580": "d97e7fd8ce5890b10d7643b186b42922bd30ae72", "1582": "14b14b9f5f08db6f77927b517fb259095f6d341f", "1583": "545807a2413a511c4f6ef64f7c679fc32f28e650", "1587": "41740a7e8ee8c9883e4b2503bec7a790d3f8b928", "1588": "f60ef8fb69e5c3e2939a0ee5d1f55d9017649a7f", "1593": "ee2f9afa39f0d730a1c82742eb8e1dc339ee41d7", "1596": "56f0282c767f98008093845581b3794a6b7b9336", "1602": "659c385a7ad98c926b69e12a4a6a88d70d25a4de", "1613": "d97f8cfecf0534e5c515675b5ca6a25cbd85fffe", "1618": "61c340a5e1634a7c94dc879413d0979e858e0298", "1619": "166f04052fd67f2d85007498c984765c5dae49a1", "1627": "c7fea67f870f334c76a7166b2f725525481fa126", "1636": "328f5450c0769fcddb1c92f4805f3ec5fba6520e", "1638": "91b1d563714b2f3c262ceb17e4f3bd8646415a33", "1639": "05fc1a7f13e8c3cf21ff671f7a942687f88c5b42", "1656": "2f69d5a0b428d1b683bdd77d2535abd31d65615a", "1668": "da288af368625f96a34167edff8a02bbfdd25c86", "1674": "24b26ea5f2230a27bbbbb6e8bfeff765100eeb83", "1675": "6b72e8652824b991eceb96aea67b2ca4ee68de3b", "1688": "a9e3e46c451bd995441ca492315a4143d0a13ae3", "1689": "46f680197a5417f549a5d2349987332e2c54b9f8", "1692": "8e8ce085853f14ee43176ee9508f5ebeb8ab2c82", "1707": "92e6877fd3f020da7119833ff2b063c524e60f39", "1712": "b36736c7d0e6a6c06b809e755964db5ca138eeef", "1721": "e5548190b6245e69a0b3423f8e755939321ddbad", "1738": "03017675d364f942c4cac4ef9dfde1a98f4d599c", "1758": "70da0f5531b9829eca85a3ea048f7db0096c5d84", "1767": "e0ced3823a8f705f5fcc295be82740e7d5105f8c", "1768": "1d6a5853d70f052a3473e2a63cae48b00b365e52"}, "revision_to_date": {"42": 1678962043000, "214": 1683553743000, "292": 1684752617000, "308": 1685003160000, "436": 1686329453000, "499": 1687514692000, "504": 1687872752000, "506": 1687875060000, "508": 1687875381000, "555": 1689023430000, "592": 1690274961000, "611": 1690383063000, "620": 1690877437000, "630": 1691153153000, "650": 1691507415000, "699": 1692029284000, "745": 1693225318000, "808": 1693832948000, "815": 1693959700000, "828": 1694183836000, "829": 1694192794000, "840": 1694691587000, "861": 1695302465000, "891": 1695645568000, "900": 1695748368000, "916": 1695993535000, "928": 1696342948000, "936": 1696861881000, "985": 1697796332000, "996": 1698065342000, "1001": 1698162437000, "1002": 1698197320000, "1004": 1698283800000, "1020": 1698344471000, "1023": 1698456439000, "1024": 1698542557000, "1029": 1698674772000, "1030": 1698689107000, "1036": 1698773717000, "1037": 1698767871000, "1039": 1698788569000, "1047": 1698844752000, "1048": 1698846082000, "1097": 1699955331000, "1099": 1699288038000, "1114": 1700074487000, "1117": 1700246378000, "1124": 1700271016000, "1125": 1700356996000, "1130": 1700486284000, "1132": 1700501108000, "1150": 1698751547000, "1157": 1700658270000, "1163": 1700745145000, "1164": 1700748344000, "1166": 1700819468000, "1169": 1700831897000, "1170": 1700875739000, "1171": 1700963086000, "1172": 1701048411000, "1187": 1701130500000, "1195": 1701208375000, "1200": 1701338845000, "1208": 1701417000000, "1218": 1701480517000, "1219": 1701566815000, "1225": 1701338857000, "1248": 1701789407000, "1255": 1701899172000, "1257": 1701788316000, "1277": 1702047809000, "1278": 1702085265000, "1279": 1702171669000, "1286": 1701705051000, "1288": 1702345523000, "1289": 1702383559000, "1299": 1702390999000, "1307": 1702566689000, "1310": 1702571706000, "1315": 1702902704000, "1317": 1702642469000, "1320": 1703070762000, "1326": 1703095157000, "1332": 1703094181000, "1333": 1703295120000, "1334": 1703380964000, "1335": 1703467570000, "1336": 1703553546000, "1337": 1703640069000, "1339": 1703770128000, "1340": 1703812926000, "1341": 1703899555000, "1342": 1703985533000, "1343": 1704072350000, "1364": 1703242398000, "1381": 1704291210000, "1383": 1704288083000, "1394": 1704398761000, "1395": 1704504556000, "1396": 1704590540000, "1403": 1704731968000, "1410": 1704734863000, "1415": 1704879725000, "1419": 1704718658000, "1430": 1705055585000, "1434": 1705070791000, "1436": 1705107122000, "1437": 1705193653000, "1444": 1705330362000, "1454": 1705325464000, "1465": 1705332414000, "1488": 1705573079000, "1516": 1705671597000, "1527": 1705712337000, "1529": 1705913450000, "1539": 1706016015000, "1543": 1706057278000, "1545": 1706109394000, "1571": 1706179066000, "1578": 1706289801000, "1579": 1706316731000, "1580": 1706402771000, "1582": 1706515616000, "1583": 1706519986000, "1587": 1706615159000, "1588": 1706624458000, "1593": 1706662467000, "1596": 1706711120000, "1602": 1706796477000, "1613": 1706885077000, "1618": 1706921497000, "1619": 1707007836000, "1627": 1707146004000, "1636": 1707226614000, "1638": 1707226614000, "1639": 1707227524000, "1656": 1707329122000, "1668": 1707396055000, "1674": 1707409018000, "1675": 1707409032000, "1688": 1707528772000, "1689": 1707615447000, "1692": 1707728577000, "1707": 1707847861000, "1712": 1707819609000, "1721": 1707920407000, "1738": 1708015048000, "1758": 1706699029000, "1767": 1708133869000, "1768": 1708220175000}, "params": {"machine": ["ArcticDB-Runner-Medium"], "python": ["3.6"], "numpy": ["", null], "pandas": ["", null], "attrs": ["", null], "dataclasses": ["", null], "protobuf": ["", null], "msgpack": ["", null], "packaging": ["", null], "pymongo": ["", null], "grpcio-tools": ["", null], "xxhash": ["", null], "six": ["", null], "pyyaml": ["", null], "decorator": ["", null], "prometheus_client": ["", null], "psutil": ["", null], "branch": ["master"]}, "graph_param_list": [{"machine": "ArcticDB-Runner-Medium", "python": "3.6", "numpy": "", "pandas": "", "attrs": "", "dataclasses": "", "protobuf": "", "msgpack": "", "packaging": "", "pymongo": "", "branch": "master", "grpcio-tools": null, "xxhash": null, "six": null, "pyyaml": null, "decorator": null, "prometheus_client": null, "psutil": null}, {"machine": "ArcticDB-Runner-Medium", "python": "3.6", "branch": "master", "numpy": null, "pandas": null, "attrs": null, "dataclasses": null, "protobuf": null, "msgpack": null, "packaging": null, "pymongo": null, "grpcio-tools": null, "xxhash": null, "six": null, "pyyaml": null, "decorator": null, "prometheus_client": null, "psutil": null}, {"machine": "ArcticDB-Runner-Medium", "python": "3.6", "numpy": "", "pandas": "", "attrs": "", "dataclasses": "", "grpcio-tools": "", "protobuf": "", "xxhash": "", "six": "", "msgpack": "", "pyyaml": "", "decorator": "", "prometheus_client": "", "packaging": "", "pymongo": "", "branch": "master", "psutil": null}, {"machine": "ArcticDB-Runner-Medium", "python": "3.6", "numpy": "", "pandas": "", "attrs": "", "dataclasses": "", "grpcio-tools": "", "protobuf": "", "xxhash": "", "six": "", "msgpack": "", "psutil": "", "pyyaml": "", "decorator": "", "prometheus_client": "", "packaging": "", "pymongo": "", "branch": "master"}], "benchmarks": {"basic_functions.BasicFunctions.peakmem_read": {"code": "class BasicFunctions:\n    def peakmem_read(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        [lib.read(f\"{sym}_sym\").data for sym in range(num_symbols)]\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))", "name": "basic_functions.BasicFunctions.peakmem_read", "param_names": ["rows", "num_symbols"], "params": [["100000", "150000"], ["500", "1000"]], "setup_cache_key": "basic_functions:23", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "c5aec6c6c0473d8bc76d9ee9e49666cb533fadc902f63da49d81d51262b51aca"}, "basic_functions.BasicFunctions.peakmem_read_batch": {"code": "class BasicFunctions:\n    def peakmem_read_batch(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n        lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))", "name": "basic_functions.BasicFunctions.peakmem_read_batch", "param_names": ["rows", "num_symbols"], "params": [["100000", "150000"], ["500", "1000"]], "setup_cache_key": "basic_functions:23", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "848aa90516ee6297965afbf0ade938753828f43537bc2e47159339fdef65751c"}, "basic_functions.BasicFunctions.peakmem_read_batch_with_columns": {"code": "class BasicFunctions:\n    def peakmem_read_batch_with_columns(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        COLS = [\"value\"]\n        read_reqs = [ReadRequest(f\"{sym}_sym\", columns=COLS) for sym in range(num_symbols)]\n        lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))", "name": "basic_functions.BasicFunctions.peakmem_read_batch_with_columns", "param_names": ["rows", "num_symbols"], "params": [["100000", "150000"], ["500", "1000"]], "setup_cache_key": "basic_functions:23", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "dd753b503b4c6466bd6c0e166abe319fab90acbe8b8b72ac219393bcce4dc480"}, "basic_functions.BasicFunctions.peakmem_read_batch_with_date_ranges": {"code": "class BasicFunctions:\n    def peakmem_read_batch_with_date_ranges(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        dr = pd.date_range(\"2023-01-01\", \"2023-01-01\")\n        read_reqs = [ReadRequest(f\"{sym}_sym\", date_range=dr) for sym in range(num_symbols)]\n        lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))", "name": "basic_functions.BasicFunctions.peakmem_read_batch_with_date_ranges", "param_names": ["rows", "num_symbols"], "params": [["100000", "150000"], ["500", "1000"]], "setup_cache_key": "basic_functions:23", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "97cf81d51ce5d7f2d72b9e1bc6bc9d201d6506fb218914dae9e872be0c590c98"}, "basic_functions.BasicFunctions.peakmem_read_short_wide": {"code": "class BasicFunctions:\n    def peakmem_read_short_wide(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        lib.read(\"short_wide_sym\").data\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))", "name": "basic_functions.BasicFunctions.peakmem_read_short_wide", "param_names": ["rows", "num_symbols"], "params": [["100000", "150000"], ["500", "1000"]], "setup_cache_key": "basic_functions:23", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "aa29d7c366133483a54489dec54422fb214b792cf58f7e74a8b803981a12168a"}, "basic_functions.BasicFunctions.peakmem_read_with_columns": {"code": "class BasicFunctions:\n    def peakmem_read_with_columns(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        COLS = [\"value\"]\n        [lib.read(f\"{sym}_sym\", columns=COLS).data for sym in range(num_symbols)]\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))", "name": "basic_functions.BasicFunctions.peakmem_read_with_columns", "param_names": ["rows", "num_symbols"], "params": [["100000", "150000"], ["500", "1000"]], "setup_cache_key": "basic_functions:23", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "a0557e06ec9ce45e6a790831b98820ab7f9e9fcdc08d1944cb7869088bc7c426"}, "basic_functions.BasicFunctions.peakmem_read_with_date_ranges": {"code": "class BasicFunctions:\n    def peakmem_read_with_date_ranges(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        dr = pd.date_range(\"2023-01-01\", \"2023-01-01\")\n        [lib.read(f\"{sym}_sym\", date_range=dr).data for sym in range(num_symbols)]\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))", "name": "basic_functions.BasicFunctions.peakmem_read_with_date_ranges", "param_names": ["rows", "num_symbols"], "params": [["100000", "150000"], ["500", "1000"]], "setup_cache_key": "basic_functions:23", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "64588b255dcadca97a49da42f6ceee52194febf23fa3da431529cab46b5bb147"}, "basic_functions.BasicFunctions.peakmem_write": {"code": "class BasicFunctions:\n    def peakmem_write(self, rows, num_symbols):\n        lib = self.get_fresh_lib()\n        for sym in range(num_symbols):\n            lib.write(f\"{sym}_sym\", self.df)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))", "name": "basic_functions.BasicFunctions.peakmem_write", "param_names": ["rows", "num_symbols"], "params": [["100000", "150000"], ["500", "1000"]], "setup_cache_key": "basic_functions:23", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "1bf92b4942f7d1013c86e5d4efd63d1fecb0ab2f2fb6f4ffa8d0790fb50f624d"}, "basic_functions.BasicFunctions.peakmem_write_batch": {"code": "class BasicFunctions:\n    def peakmem_write_batch(self, rows, num_symbols):\n        lib = self.get_fresh_lib()\n        df = self.df\n        payloads = [WritePayload(f\"{sym}_sym\", df) for sym in range(num_symbols)]\n        lib.write_batch(payloads)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))", "name": "basic_functions.BasicFunctions.peakmem_write_batch", "param_names": ["rows", "num_symbols"], "params": [["100000", "150000"], ["500", "1000"]], "setup_cache_key": "basic_functions:23", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "ac9541769beec9d47bb64406600e7a610a122fb187f28e8a5903526bf16014a1"}, "basic_functions.BasicFunctions.peakmem_write_short_wide": {"code": "class BasicFunctions:\n    def peakmem_write_short_wide(self, rows, num_symbols):\n        lib = self.get_fresh_lib()\n        lib.write(\"short_wide_sym\", self.df_short_wide)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))", "name": "basic_functions.BasicFunctions.peakmem_write_short_wide", "param_names": ["rows", "num_symbols"], "params": [["100000", "150000"], ["500", "1000"]], "setup_cache_key": "basic_functions:23", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "e1f4298bd5e6c512f0b5a7be2916ddbf20d256d4ef043d4a30d27ea7720bfd4b"}, "basic_functions.BasicFunctions.peakmem_write_staged": {"code": "class BasicFunctions:\n    def peakmem_write_staged(self, rows, num_symbols):\n        lib = self.get_fresh_lib()\n        for sym in range(num_symbols):\n            lib.write(f\"{sym}_sym\", self.df, staged=True)\n    \n        for sym in range(num_symbols):\n            lib._nvs.compact_incomplete(f\"{sym}_sym\", False, False)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))", "name": "basic_functions.BasicFunctions.peakmem_write_staged", "param_names": ["rows", "num_symbols"], "params": [["100000", "150000"], ["500", "1000"]], "setup_cache_key": "basic_functions:23", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "d9b981399b4f03502d93f4fd607918960ee468ec0a7ee821121e13c9624b0217"}, "basic_functions.BasicFunctions.time_read": {"code": "class BasicFunctions:\n    def time_read(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        [lib.read(f\"{sym}_sym\").data for sym in range(num_symbols)]\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_read", "number": 5, "param_names": ["rows", "num_symbols"], "params": [["100000", "150000"], ["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:23", "timeout": 6000, "type": "time", "unit": "seconds", "version": "f14eee77b5815fed53f18934d2221bdad341961c5e1c66145737b9c7369f90de", "warmup_time": -1}, "basic_functions.BasicFunctions.time_read_batch": {"code": "class BasicFunctions:\n    def time_read_batch(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n        lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_read_batch", "number": 5, "param_names": ["rows", "num_symbols"], "params": [["100000", "150000"], ["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:23", "timeout": 6000, "type": "time", "unit": "seconds", "version": "1105c8663471c2de9d44562f639dca10115b5389148955fb3faee0fd754949ee", "warmup_time": -1}, "basic_functions.BasicFunctions.time_read_batch_pure": {"code": "class BasicFunctions:\n    def time_read_batch_pure(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        lib.read_batch(self.read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_read_batch_pure", "number": 5, "param_names": ["rows", "num_symbols"], "params": [["100000", "150000"], ["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:23", "timeout": 6000, "type": "time", "unit": "seconds", "version": "dce09f508518caed1d80fde228523d4e92a8207a5ba8acc66a6cf0ed30999f06", "warmup_time": -1}, "basic_functions.BasicFunctions.time_read_batch_with_columns": {"code": "class BasicFunctions:\n    def time_read_batch_with_columns(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        COLS = [\"value\"]\n        read_reqs = [ReadRequest(f\"{sym}_sym\", columns=COLS) for sym in range(num_symbols)]\n        lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_read_batch_with_columns", "number": 5, "param_names": ["rows", "num_symbols"], "params": [["100000", "150000"], ["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:23", "timeout": 6000, "type": "time", "unit": "seconds", "version": "55e242190d9bee2bd297a66069f805d7d2a3bed04c751a80f3495057f41d0d88", "warmup_time": -1}, "basic_functions.BasicFunctions.time_read_batch_with_date_ranges": {"code": "class BasicFunctions:\n    def time_read_batch_with_date_ranges(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        dr = pd.date_range(\"2023-01-01\", \"2023-01-01\")\n        read_reqs = [ReadRequest(f\"{sym}_sym\", date_range=dr) for sym in range(num_symbols)]\n        lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_read_batch_with_date_ranges", "number": 5, "param_names": ["rows", "num_symbols"], "params": [["100000", "150000"], ["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:23", "timeout": 6000, "type": "time", "unit": "seconds", "version": "917b7be653cd4752d70f0764766ea1e85ad305338e3c6352a9923253cc7de3e3", "warmup_time": -1}, "basic_functions.BasicFunctions.time_read_short_wide": {"code": "class BasicFunctions:\n    def time_read_short_wide(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        lib.read(\"short_wide_sym\").data\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_read_short_wide", "number": 5, "param_names": ["rows", "num_symbols"], "params": [["100000", "150000"], ["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:23", "timeout": 6000, "type": "time", "unit": "seconds", "version": "7a8aebb9bfb5ae861080001e5e2dfe5dc5ec6f9cb0a18c5320e7228b48fd991d", "warmup_time": -1}, "basic_functions.BasicFunctions.time_read_with_columns": {"code": "class BasicFunctions:\n    def time_read_with_columns(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        COLS = [\"value\"]\n        [lib.read(f\"{sym}_sym\", columns=COLS).data for sym in range(num_symbols)]\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_read_with_columns", "number": 5, "param_names": ["rows", "num_symbols"], "params": [["100000", "150000"], ["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:23", "timeout": 6000, "type": "time", "unit": "seconds", "version": "a7de9888e084e882f1b8b694824aefa9a1b393ac921e98ec5b781441f782dc9e", "warmup_time": -1}, "basic_functions.BasicFunctions.time_read_with_date_ranges": {"code": "class BasicFunctions:\n    def time_read_with_date_ranges(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        dr = pd.date_range(\"2023-01-01\", \"2023-01-01\")\n        [lib.read(f\"{sym}_sym\", date_range=dr).data for sym in range(num_symbols)]\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_read_with_date_ranges", "number": 5, "param_names": ["rows", "num_symbols"], "params": [["100000", "150000"], ["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:23", "timeout": 6000, "type": "time", "unit": "seconds", "version": "b1f745f91cfe4a2a886688038612d4c8190f1d16d6b725e8c3ee8c3dd20569a8", "warmup_time": -1}, "basic_functions.BasicFunctions.time_write": {"code": "class BasicFunctions:\n    def time_write(self, rows, num_symbols):\n        lib = self.get_fresh_lib()\n        for sym in range(num_symbols):\n            lib.write(f\"{sym}_sym\", self.df)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_write", "number": 5, "param_names": ["rows", "num_symbols"], "params": [["100000", "150000"], ["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:23", "timeout": 6000, "type": "time", "unit": "seconds", "version": "b4e20bcf340f05bcbe839792dedccd04ca7c61c04481997a7a1f3a91b3435125", "warmup_time": -1}, "basic_functions.BasicFunctions.time_write_batch": {"code": "class BasicFunctions:\n    def time_write_batch(self, rows, num_symbols):\n        lib = self.get_fresh_lib()\n        df = self.df\n        payloads = [WritePayload(f\"{sym}_sym\", df) for sym in range(num_symbols)]\n        lib.write_batch(payloads)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_write_batch", "number": 5, "param_names": ["rows", "num_symbols"], "params": [["100000", "150000"], ["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:23", "timeout": 6000, "type": "time", "unit": "seconds", "version": "8bddad28073908b80c7780eb3e0c76740fde9e88ab261961e4501be5227c12e3", "warmup_time": -1}, "basic_functions.BasicFunctions.time_write_short_wide": {"code": "class BasicFunctions:\n    def time_write_short_wide(self, rows, num_symbols):\n        lib = self.get_fresh_lib()\n        lib.write(\"short_wide_sym\", self.df_short_wide)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_write_short_wide", "number": 5, "param_names": ["rows", "num_symbols"], "params": [["100000", "150000"], ["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:23", "timeout": 6000, "type": "time", "unit": "seconds", "version": "2088fffc825de2bcd9d28471c4d7092c42635cf1a3661e5739c412ceb2bf16e6", "warmup_time": -1}, "basic_functions.BasicFunctions.time_write_staged": {"code": "class BasicFunctions:\n    def time_write_staged(self, rows, num_symbols):\n        lib = self.get_fresh_lib()\n        for sym in range(num_symbols):\n            lib.write(f\"{sym}_sym\", self.df, staged=True)\n    \n        for sym in range(num_symbols):\n            lib._nvs.compact_incomplete(f\"{sym}_sym\", False, False)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_write_staged", "number": 5, "param_names": ["rows", "num_symbols"], "params": [["100000", "150000"], ["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:23", "timeout": 6000, "type": "time", "unit": "seconds", "version": "b72c33b6e6b44ecdbef7f56a9ab4de3c76137d937f2ced3fd39d066f309c94e5", "warmup_time": -1}, "list_functions.ListFunctions.peakmem_list_symbols": {"code": "class ListFunctions:\n    def peakmem_list_symbols(self, num_symbols):\n        lib = self.ac[f\"{num_symbols}_num_symbols\"]\n        lib.list_symbols()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            self.ac.create_library(lib_name)\n            lib = self.ac[lib_name]\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))", "name": "list_functions.ListFunctions.peakmem_list_symbols", "param_names": ["num_symbols"], "params": [["500", "1000"]], "setup_cache_key": "list_functions:22", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "0d50729844beae58e9cba083ee7214a55e5c1639b8439842ed1cdf080b7b2a4f"}, "list_functions.ListFunctions.peakmem_list_versions": {"code": "class ListFunctions:\n    def peakmem_list_versions(self, num_symbols):\n        lib = self.ac[f\"{num_symbols}_num_symbols\"]\n        lib.list_versions()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            self.ac.create_library(lib_name)\n            lib = self.ac[lib_name]\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))", "name": "list_functions.ListFunctions.peakmem_list_versions", "param_names": ["num_symbols"], "params": [["500", "1000"]], "setup_cache_key": "list_functions:22", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "c317f2aa935e2b739a1a232439412ad1da88ee9eb332daf0f560e00ec27f1143"}, "list_functions.ListFunctions.time_has_symbol": {"code": "class ListFunctions:\n    def time_has_symbol(self, num_symbols):\n        lib = self.ac[f\"{num_symbols}_num_symbols\"]\n        lib.has_symbol(\"250_sym\")\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            self.ac.create_library(lib_name)\n            lib = self.ac[lib_name]\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))", "min_run_count": 2, "name": "list_functions.ListFunctions.time_has_symbol", "number": 5, "param_names": ["num_symbols"], "params": [["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "list_functions:22", "timeout": 6000, "type": "time", "unit": "seconds", "version": "8a8e9e8927b3b8698d1ef4bab4119455588fc62a3bbfa62321e6ce83f5f428ad", "warmup_time": -1}, "list_functions.ListFunctions.time_list_symbols": {"code": "class ListFunctions:\n    def time_list_symbols(self, num_symbols):\n        lib = self.ac[f\"{num_symbols}_num_symbols\"]\n        lib.list_symbols()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            self.ac.create_library(lib_name)\n            lib = self.ac[lib_name]\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))", "min_run_count": 2, "name": "list_functions.ListFunctions.time_list_symbols", "number": 5, "param_names": ["num_symbols"], "params": [["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "list_functions:22", "timeout": 6000, "type": "time", "unit": "seconds", "version": "c91930aa368c8d3ad7bd56d7f827edcca3f16645c205bf6e042c57a5792a7113", "warmup_time": -1}, "list_functions.ListFunctions.time_list_versions": {"code": "class ListFunctions:\n    def time_list_versions(self, num_symbols):\n        lib = self.ac[f\"{num_symbols}_num_symbols\"]\n        lib.list_versions()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            self.ac.create_library(lib_name)\n            lib = self.ac[lib_name]\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))", "min_run_count": 2, "name": "list_functions.ListFunctions.time_list_versions", "number": 5, "param_names": ["num_symbols"], "params": [["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "list_functions:22", "timeout": 6000, "type": "time", "unit": "seconds", "version": "ed4587ca351d24477560baa8e184e265b612140183e88c412c36bf8a9352191c", "warmup_time": -1}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_numeric": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_filtering_numeric(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 10.0]\n        lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_numeric", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:21", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "119932c4738eb092fbbd9da22b3ff1e16bd4d6052dace2da6748fff5b416462d"}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_string_isin": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_filtering_string_isin(self, num_rows):\n        lib = self.ac[self.lib_name]\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_string_isin", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:21", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "e77e28f6a2cb96e5b549ca58b551462e0ff90d7704ad3cf12f3e2ad71b1ccd73"}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_projection": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_projection(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        lib.read(f\"{num_rows}_rows\", columns=[\"new_col\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_projection", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:21", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "928c58e85b09b5969e7b2302b3a3a3c5af5d47ce5bd1cd5245387bf34fcb266b"}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_1": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_1(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_1", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:21", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "a1d501b169474fd16fdb1e90760157944483bba8d81eb3cb5791bbd487706ef6"}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_3": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_3(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_3", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:21", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "0b19c5c0e4be8742e99e32f13332544c343ac260474ee51bfb8911225c90b87b"}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_4": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_4(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_4", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:21", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "76d23e33cb5a319de6d0e007054bc0c3588cde135a08797e8f0c84493ed30189"}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_adv_query_2": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_adv_query_2(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_adv_query_2", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:21", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "36489c1cb0a3965ff6003b10a5f5cf2f2354a619346e53c488b2a3d1a07fd4c2"}, "local_query_builder.LocalQueryBuilderFunctions.time_filtering_numeric": {"code": "class LocalQueryBuilderFunctions:\n    def time_filtering_numeric(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 1.0]\n        lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_filtering_numeric", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "local_query_builder:21", "timeout": 6000, "type": "time", "unit": "seconds", "version": "441c9db2c41da7c09c3f758c1cd0993dc5ea674f05d708e8997567826a961251", "warmup_time": -1}, "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_isin": {"code": "class LocalQueryBuilderFunctions:\n    def time_filtering_string_isin(self, num_rows):\n        lib = self.ac[self.lib_name]\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_isin", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "local_query_builder:21", "timeout": 6000, "type": "time", "unit": "seconds", "version": "f9d819943b527f80d002995694ba8cfc60f5ed75586144f6c4d1ba612a761294", "warmup_time": -1}, "local_query_builder.LocalQueryBuilderFunctions.time_projection": {"code": "class LocalQueryBuilderFunctions:\n    def time_projection(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        lib.read(f\"{num_rows}_rows\", columns=[\"new_col\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_projection", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "local_query_builder:21", "timeout": 6000, "type": "time", "unit": "seconds", "version": "911828bde306e54bd165122188fb8acc406697e8cc94daa3e02596c1efc4530a", "warmup_time": -1}, "local_query_builder.LocalQueryBuilderFunctions.time_query_1": {"code": "class LocalQueryBuilderFunctions:\n    def time_query_1(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_1", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "local_query_builder:21", "timeout": 6000, "type": "time", "unit": "seconds", "version": "e0d8b8b13b6012a8fe388a57e1d816fc387312c6595ff5cbf0490ae32a11642e", "warmup_time": -1}, "local_query_builder.LocalQueryBuilderFunctions.time_query_3": {"code": "class LocalQueryBuilderFunctions:\n    def time_query_3(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_3", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "local_query_builder:21", "timeout": 6000, "type": "time", "unit": "seconds", "version": "22c320dbb7f3915712bffffef0c86ec9a51a167a8aee1af369ae9afceb47ad0b", "warmup_time": -1}, "local_query_builder.LocalQueryBuilderFunctions.time_query_4": {"code": "class LocalQueryBuilderFunctions:\n    def time_query_4(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_4", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "local_query_builder:21", "timeout": 6000, "type": "time", "unit": "seconds", "version": "f6c81bfbe6bd6a3ed5de7ede40c048ee2e7f0f2f844608169ddf1768a18aa900", "warmup_time": -1}, "local_query_builder.LocalQueryBuilderFunctions.time_query_adv_query_2": {"code": "class LocalQueryBuilderFunctions:\n    def time_query_adv_query_2(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_adv_query_2", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "local_query_builder:21", "timeout": 6000, "type": "time", "unit": "seconds", "version": "8167772e10cd53cd18fa670b57287145bdee6a010f1f8e07c9beec045706692e", "warmup_time": -1}, "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_1": {"code": "class PersistentQueryBuilderFunctions:\n    def time_query_1(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        pass\n\n    def setup_cache(self):\n        self.ac = real_s3_from_environment_variables(shared_path=True).create_fixture().create_arctic()\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = \"query_builder_benchmark_lib\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_1", "number": 2, "param_names": ["param1"], "params": [["10000000", "100000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "persistent_query_builder:28", "timeout": 6000, "type": "time", "unit": "seconds", "version": "420c3ba188fd433b786e4be9cdb08340590d4595ac3a98badd6ebe63c41e7ecf", "warmup_time": -1}, "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_3": {"code": "class PersistentQueryBuilderFunctions:\n    def time_query_3(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        pass\n\n    def setup_cache(self):\n        self.ac = real_s3_from_environment_variables(shared_path=True).create_fixture().create_arctic()\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = \"query_builder_benchmark_lib\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_3", "number": 2, "param_names": ["param1"], "params": [["10000000", "100000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "persistent_query_builder:28", "timeout": 6000, "type": "time", "unit": "seconds", "version": "c835f67ed597ec88b640012a8a87cd4fd26b5b208db2812274c5e90a0d7aa128", "warmup_time": -1}, "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_4": {"code": "class PersistentQueryBuilderFunctions:\n    def time_query_4(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        pass\n\n    def setup_cache(self):\n        self.ac = real_s3_from_environment_variables(shared_path=True).create_fixture().create_arctic()\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = \"query_builder_benchmark_lib\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_4", "number": 2, "param_names": ["param1"], "params": [["10000000", "100000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "persistent_query_builder:28", "timeout": 6000, "type": "time", "unit": "seconds", "version": "aa0500f4f7201412558afeb82ada834bd10d4b56beae1dc16f73ccec69b33d45", "warmup_time": -1}, "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_adv_query_2": {"code": "class PersistentQueryBuilderFunctions:\n    def time_query_adv_query_2(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        pass\n\n    def setup_cache(self):\n        self.ac = real_s3_from_environment_variables(shared_path=True).create_fixture().create_arctic()\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = \"query_builder_benchmark_lib\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_adv_query_2", "number": 2, "param_names": ["param1"], "params": [["10000000", "100000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "persistent_query_builder:28", "timeout": 6000, "type": "time", "unit": "seconds", "version": "827ddb04703d351338fde3b2a7eb5f147fa81c5269b630ba32554d8aacb396bc", "warmup_time": -1}}, "machines": {"ArcticDB-Runner-Medium": {"machine": "ArcticDB-Runner-Medium", "version": 1}}, "tags": {"1.0.1": 42, "v1.0.1": 42, "v1.1.0": 214, "v1.2.0": 292, "v1.2.1": 308, "v1.3.0": 436, "v1.4.0": 499, "v1.4.1": 504, "v1.4.1-pre.seatontst": 506, "v1.4.1-pre.seatontst.2": 508, "v1.5.0": 555, "v1.6.0": 592, "v1.6.1": 650, "v1.6.1-rc0": 611, "v1.6.1-rc1": 620, "v1.6.1rc2": 630, "v1.6.2": 928, "v1.6.2rc0": 916, "v2.0.0": 745, "v2.0.0rc0": 699, "v3.0.0": 828, "v3.0.0rc0": 808, "v3.0.0rc1": 815, "v3.0.0rc2": 829, "v4.0.0": 900, "v4.0.0rc0": 840, "v4.0.0rc1": 861, "v4.0.0rc2": 891, "v4.0.1": 985, "v4.0.2": 1047, "v4.0.2rc0": 1036, "v4.0.3": 1163, "v4.1.0": 1048, "v4.1.0-docs": 1132, "v4.1.0rc0": 936, "v4.1.0rc1": 1030, "v4.1.0rc2": 1039, "v4.2.0": 1289, "v4.2.0-docs": 1289, "v4.2.0rc0": 1169, "v4.2.1": 1307, "v4.2.1-docs": 1444, "v4.3.0": 1638, "v4.3.0-docs": 1588, "v4.3.0rc0": 1545, "v4.3.0rc1": 1583, "v4.3.0rc2": 1639, "v4.3.1": 1675, "v4.3.1-docs": 1712, "vtop_level_imports-docs": 1430}, "pages": [["", "Grid view", "Display as a agrid"], ["summarylist", "List view", "Display as a list"], ["regressions", "Show regressions", "Display information about recent regressions"]]}